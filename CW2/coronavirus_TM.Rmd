---
title: "coronavirus_TM"
author: "Xinang Hu"
date: "3/12/2020"
output: pdf_document
---

##1. load data

```{r}
#read in the data
textdata1 <- read.csv('mar15.csv', encoding = "UTF-8")
textdata2 <- read.csv('mar18.csv', encoding = "UTF-8")
textdata3 <- read.csv('mar21.csv', encoding = "UTF-8")
textdata4 <- read.csv('mar24.csv', encoding = "UTF-8")
textdata5 <- read.csv('mar27.csv', encoding = "UTF-8")
textdata6 <- read.csv('mar30.csv', encoding = "UTF-8")
Alltextdata <- rbind(textdata1,textdata2,textdata3,textdata4,textdata5,textdata6)
```

##2. data cleaning

```{r, eval=True}
# install these packages if you do not have them installed yet
# install.packages("twitteR")
# install.packages("plyr")
# install.packages("stringr")
# install.packages(tm)

# load the libraries
library(twitteR)
library(plyr)
library(stringr)
library(ggplot2)
library(tm)


# import your data set to analyse,
# ensure it is in the same directory as your code, otherwise you need to add the path
# dataset='mar15.csv'
# Dataset2 <- read.csv(dataset)
tweets.df <- Alltextdata$text


#View(tweets.df)


# get rid of problem characters
tweets.df <- sapply(tweets.df,function(row) iconv(row, "latin1", "ASCII", sub=""))
# convert text to lowercase
tweets.df<-tolower(tweets.df)

# remove punctuation, digits, special characters etc
tweets.df = gsub("&amp", "", tweets.df)
tweets.df= gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweets.df)
tweets.df = gsub("@\\w+", "", tweets.df)
tweets.df= gsub("[[:punct:]]", "", tweets.df)
tweets.df = gsub("[[:digit:]]", "", tweets.df)
tweets.df = gsub("http\\w+", "", tweets.df)
tweets.df = gsub("[ \t]{2,}", "", tweets.df)
tweets.df= gsub("^\\s+|\\s+$", "", tweets.df) 
tweets.df= gsub("rt", "", tweets.df)


# get rid of unnecessary spaces
tweets.df <- str_replace_all(tweets.df," "," ")

# take out the retweet header
tweets.df <- str_replace(tweets.df,"RT @[a-z,A-Z]*: ","")

# get rid of hashtags
tweets.df <- str_replace_all(tweets.df,"#[a-z,A-Z]*","")

# get rid of references to other screen names
tweets.df <- str_replace_all(tweets.df,"@[a-z,A-Z]*","") 
tweets.df<-data.frame(text=tweets.df)

tweets.time <- Alltextdata$created
tweets.df=cbind(tweets.df,tweets.time)
names(tweets.df) <- c("text", "created")
write.csv(tweets.df, file="clean.csv")
#textdata<-tweets.df
#View(tweets.df)

```
## 3. term frequency change with date
```{r, eval= TRUE, message=FALSE}
library(slam)
library(tm)
library(lubridate)
library(syuzhet)
library(dplyr)
library("reshape2")
library("ggplot2")

textdata <- read.csv('clean.csv')
#convert the twitter data format
textdata$created <- as.POSIXct(textdata$created, tz="GMT")

#select day
textdata$day <- format(as.Date(textdata$created), "%d")

#take the text column and convert to a corpus
textdata$doc_id<-textdata$doc_id <- seq_len(nrow(textdata))  # include the doc_id
#text<as.character(textdata$text)
corpus <- Corpus(DataframeSource(textdata))
corpus <- Corpus(DataframeSource(textdata))
#corpus <- tm_map(corpus, content_transformer(tolower))
#corpus <- tm_map(corpus, removeWords, stopwords("en"))
#corpus <- tm_map(corpus, removePunctuation, preserve_intra_word_dashes = TRUE)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, stripWhitespace)

#form a document term matrix
DTM <- DocumentTermMatrix(corpus)


#select the terms you want to observe
terms_to_observe <- c( "hospital",'death','hands','paper','lockdown','china','nhs','cdc')#############
#reduce the DTM to contain only those terms
DTM_reduced <- as.matrix(DTM[, terms_to_observe])
#sum the frequecies 
counts_per_day<- aggregate(DTM_reduced, by = list(decade = textdata$day), sum)

counts_per_day_long <- melt(counts_per_day, id="decade")  # convert to long format

#Visualize the word frequecy time series
p2 <- ggplot(data = counts_per_day_long, aes(x = factor(decade), y = value, colour = variable)) +       
    geom_line(aes(group = variable)) + geom_point() + xlab("Day in Mar") +
  ylab("Frequency") +  labs(color='Terms to observe') 

p2

```

##4. polarity+topic

```{r, eval= TRUE, message=FALSE}
#install.packages("twitteR")
#install.packages("plyr")
#install.packages("stringr")
#install.packages("tm")
#install.packages("scales")

#install.packages("LDAvis")
#install.packages("tm")
#install.packages("lda")
#install.packages("servr")
#install.packages("shiny")
#install.packages("stringr")



#loading the library
library(plyr)
library(stringr)
library(ggplot2)
library(tm)
library(scales)

library(LDAvis)
library(tm)
library(lda)
library(shiny)
library(stringr)





#read in the file
textdata<-read.csv("clean.csv")
tweets.df<-textdata$text
tweets.df<-tolower(tweets.df)


#tweets.df <- sapply(tweets.df,function(row) iconv(row, "latin1", "ASCII", sub=""))

#cleaning the tweets
# tweets.df = gsub("&amp", "", tweets.df)
# tweets.df= gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweets.df)
# tweets.df = gsub("@\\w+", "", tweets.df)
# tweets.df= gsub("[[:punct:]]", "", tweets.df)
# tweets.df = gsub("[[:digit:]]", "", tweets.df)
# tweets.df = gsub("http\\w+", "", tweets.df)
# tweets.df = gsub("[ \t]{2,}", "", tweets.df)
# tweets.df= gsub("^\\s+|\\s+$", "", tweets.df) 
# 
# 
# #get rid of unnecessary spaces
# tweets.df <- str_replace_all(tweets.df," "," ")
# # Get rid of URLs
# #tweets.df <- str_replace_all(tweets.df, "http://t.co/[a-z,A-Z,0-9]*{8}","")
# # Take out retweet header, there is only one
# tweets.df <- str_replace(tweets.df,"RT @[a-z,A-Z]*: ","")
# # Get rid of hashtags
# tweets.df <- str_replace_all(tweets.df,"#[a-z,A-Z]*","")
# # Get rid of references to other screennames
# tweets.df <- str_replace_all(tweets.df,"@[a-z,A-Z]*","")  

#view cleaned tweets
#View(tweets.df)


#Reading the Lexicon positive and negative words
pos <- readLines("positive_words.txt")
neg <- readLines("negative_words.txt")

#function to calculate sentiment score
score.sentiment <- function(sentences, pos.words, neg.words, .progress='none')
{
  # Parameters
  # sentences: vector of text to score
  # pos.words: vector of words of postive sentiment
  # neg.words: vector of words of negative sentiment
  # .progress: passed to laply() to control of progress bar
  
  # create simple array of scores with laply
  scores <- laply(sentences,
                  function(sentence, pos.words, neg.words)
                  {
                    # remove punctuation
                    sentence <- gsub("[[:punct:]]", "", sentence)
                    # remove control characters
                    sentence <- gsub("[[:cntrl:]]", "", sentence)
                    # remove digits
                    sentence <- gsub('\\d+', '', sentence)
                    
                    #convert to lower
                    sentence <- tolower(sentence)
                    
                    
                    # split sentence into words with str_split (stringr package)
                    word.list <- str_split(sentence, "\\s+")
                    words <- unlist(word.list)
                    
                    # compare words to the dictionaries of positive & negative terms
                    pos.matches <- match(words, pos)
                    neg.matches <- match(words, neg)
                    
                    # get the position of the matched term or NA
                    # we just want a TRUE/FALSE
                    pos.matches <- !is.na(pos.matches)
                    neg.matches <- !is.na(neg.matches)
                    
                    # final score
                    score <- sum(pos.matches) - sum(neg.matches)
                    return(score)
                  }, pos.words, neg.words, .progress=.progress )
  # data frame with scores for each sentence
  scores.df <- data.frame(text=sentences, score=scores)
  return(scores.df)
}
#sentiment score
scores_twitter <- score.sentiment(tweets.df, pos.txt, neg.txt, .progress='text')


#View(scores_twitter)

#Summary of the sentiment scores
summary(scores_twitter)

scores_twitter$score_chr <- ifelse(scores_twitter$score < 0,'Negtive', ifelse(scores_twitter$score > 0, 'Positive', 'Neutral'))
scores_twitter$negtive <- ifelse(scores_twitter$score < 0,as.character(textdata$text), "")

#View(scores_twitter)


#Convert score_chr to factor for visualizations
scores_twitter$score_chr <- as.factor(scores_twitter$score_chr)
names(scores_twitter)[3]<-paste("Sentiment")  

#plot to show number of negative, positive and neutral comments
Viz1 <- ggplot(scores_twitter, aes(x=Sentiment, fill=Sentiment))+ geom_bar(aes(y = (..count..)/sum(..count..))) + 
  scale_y_continuous(labels = percent)+labs(y="Score")+
  theme(text =element_text(size=15))+theme(axis.text = element_text(size=15))+ theme(legend.position="none")+ coord_cartesian(ylim=c(0,0.6)) + scale_fill_manual(values=c("firebrick1", "grey50", "limeGREEN"))
Viz1



stop_words <- stopwords("SMART")

Dataset2<-read.csv("clean.csv")
tweet <- Dataset2$text


tweet <- sapply(tweet, function(x) iconv(x, to='UTF-8', sub='byte'))


tweet= gsub("[[:punct:]]", "", tweet)
tweet = gsub("[[:digit:]]", "", tweet)
tweet= gsub("http\\w+", "", tweet)
tweet = gsub("[ \t]{2,}", "", tweet)
tweet= gsub("^\\s+|\\s+$", "", tweet)
#ref: ( Hicks , 2014)

#get rid of unnecessary spaces
tweet <- str_replace_all(tweet," "," ")

tweet <- str_replace(tweet,"RT @[a-z,A-Z]*: ","")
# Get rid of hashtags
tweet <- str_replace_all(tweet,"#[a-z,A-Z]*","")
# Get rid of references to other screennames
tweet<- str_replace_all(tweet,"@[a-z,A-Z]*","")

# tokenize on space and output as a list:
doc.list <- strsplit(tweet, "[[:space:]]+")

# compute the table of terms:
term.table <- table(unlist(doc.list))
term.table <- sort(term.table, decreasing = TRUE)

# remove terms that are stop words or occur fewer than 5 times:
del <- names(term.table) %in% stop_words | term.table < 5
term.table <- term.table[!del]
vocab <- names(term.table)

# now put the documents into the format required by the lda package:
get.terms <- function(x) {
  index <- match(x, vocab)
  index <- index[!is.na(index)]
  rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents <- lapply(doc.list, get.terms)

# Compute some statistics related to the data set:
D <- length(documents)  # number of documents 
W <- length(vocab)  # number of terms in the vocab 
doc.length <- sapply(documents, function(x) sum(x[2, ]))  # number of tokens per document 
N <- sum(doc.length)  # total number of tokens in the data 
term.frequency <- as.integer(term.table)  # frequencies of terms in the corpus 


# MCMC and model tuning parameters:
K <- 20
G <- 5000
alpha <- 0.02
eta <- 0.02

# Fit the model:
library(lda)
set.seed(357)
t1 <- Sys.time()
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = 5, vocab = vocab, 
                                   num.iterations = 200, alpha = 0.5, eta=0.5,
                                    initial = NULL, burnin = 0,
                                   compute.log.likelihood = TRUE)

t2 <- Sys.time()
t2 - t1  

#LDAvis
theta <- t(apply(fit$document_sums + 0.5, 2, function(x) x/sum(x)))
phi <- t(apply(t(fit$topics) + 0.5, 2, function(x) x/sum(x)))


tweetvis <- list(phi = phi,
                     theta = theta,
                     doc.length = doc.length,
                     vocab = vocab,
                     term.frequency = term.frequency)


# create the JSON object to feed the visualization:
json <- createJSON(phi = tweetvis$phi, 
                   theta = tweetvis$theta, 
                   doc.length = tweetvis$doc.length, 
                   vocab = tweetvis$vocab, 
                   term.frequency = tweetvis$term.frequency)
serVis(json, out.dir = tempfile(), open.browser = interactive())

```


##5 emotion+cloud

```{r, eval= TRUE, message=FALSE}
# load the libraries
library(syuzhet)
library(plotly)
library(tm)
library(wordcloud)
library(plyr)
library(stringr)

#import your dataset to analyse, 
#ensure it is in the same directory as your code, 
#otherwise you need to add the path
  
tweets <- read.csv('clean.csv')
  clean_tweets = tweets$text
  
  
  clean_tweets = gsub('(RT|via)((?:\\b\\W*@\\w+)+)', '', clean_tweets)
  # #clean_tweets = sapply(tweets, function(x) x$getText())
  # # remove retweet entities
  # clean_tweets = gsub('(RT|via)((?:\\b\\W*@\\w+)+)', '', clean_tweets)
  # # remove at people
  # clean_tweets = gsub('@\\w+', '', clean_tweets)
  # # remove punctuation
  # clean_tweets = gsub('[[:punct:]]', '', clean_tweets)
  # # remove numbers
  # clean_tweets = gsub('[[:digit:]]', '', clean_tweets)
  # # remove html links
  # clean_tweets = gsub('http\\w+', '', clean_tweets)
  # # remove unnecessary spaces
  # clean_tweets = gsub('[ \t]{2,}', '', clean_tweets)
  # clean_tweets = gsub('^\\s+|\\s+$', '', clean_tweets)
  # # remove emojis or special characters
  # clean_tweets = gsub('<.*>', '', enc2native(clean_tweets))
  # 
  # clean_tweets = tolower(clean_tweets)
  

  
  emotions <- get_nrc_sentiment(clean_tweets)
  emo_bar = colSums(emotions)
  emo_sum = data.frame(count=emo_bar, emotion=names(emo_bar))
  emo_sum$emotion = factor(emo_sum$emotion, levels=emo_sum$emotion[order(emo_sum$count, decreasing = TRUE)])
  
  emo_sum <- emo_sum[1:8,]
  emo_sum$percent<-(emo_sum$count/sum(emo_sum$count))*100
  
   #Visualize the emotions from NRC sentiments
plot_ly(emo_sum, x=~emotion, y=~percent, type="bar", color=~emotion) %>%
layout(xaxis=list(title=""),  yaxis = list(title = "Emotion count"),
showlegend=FALSE,title="Distribution of emotion categories") %>%
layout(yaxis = list(ticksuffix = "%"))

l1=length(emotions)
l2=length(emotions[,1])
text=tweets$text

emoList<- list()
for (i in 1:l1){
  emoText=vector()
  index=1
  for (j in 1:l2){
    if(emotions[j,i]>1){
      emoText[index]=as.character(text[j])
      index=index+1
    }
  }
  if(length(emoText)>0){
    emoList[i]<-list(emoText)
  }
}

anger=emoList[1]
sad=emoList[6]



getWordcloud <- function(emotionText ){
  tweets.df<- emotionText
  tweets.df <- sapply(tweets.df,function(row) iconv(row, "latin1", "ASCII", sub=""))
  
  # tweets.df = gsub("&amp", "", tweets.df)
  # tweets.df = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweets.df)
  # tweets.df = gsub("@\\w+", "", tweets.df)
  # tweets.df = gsub("[[:punct:]]", "", tweets.df)
  # tweets.df = gsub("[[:digit:]]", "", tweets.df)
  # tweets.df = gsub("http\\w+", "", tweets.df)
  # tweets.df = gsub("[ \t]{2,}", "", tweets.df)
  # tweets.df = gsub("^\\s+|\\s+$", "", tweets.df) 
   tweets.df = gsub("coronavirus", "", tweets.df) 
  
  # corpus will hold a collection of text documents
  tweet_corpus <- Corpus(VectorSource(tweets.df)) 
  tweet_corpus
  inspect(tweet_corpus[1])
  
  # clean text
  tweet_clean <- tm_map(tweet_corpus, removePunctuation)
  tweet_clean <- tm_map(tweet_clean, removeWords, stopwords("english"))
  tweet_clean <- tm_map(tweet_clean, removeNumbers)
  tweet_clean <- tm_map(tweet_clean, stripWhitespace)
  
  return(wordcloud(tweet_clean, random.order=0.5,max.words=100, col=rainbow(50),min.freq = 5,  scale=c(2.0,0.3)))
}

getWordcloud(anger)
getWordcloud(sad)


# #library(wordcloud2)
# 
# # get the text column
# ##dataset='mar15.csv'
# ##Dataset2 <- read.csv(dataset)
# ##tweets.df<- Dataset2$text
# tweets.df<- sad
# tweets.df <- sapply(tweets.df,function(row) iconv(row, "latin1", "ASCII", sub=""))
# 
# # tweets.df = gsub("&amp", "", tweets.df)
# # tweets.df = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweets.df)
# # tweets.df = gsub("@\\w+", "", tweets.df)
# # tweets.df = gsub("[[:punct:]]", "", tweets.df)
# # tweets.df = gsub("[[:digit:]]", "", tweets.df)
# # tweets.df = gsub("http\\w+", "", tweets.df)
# # tweets.df = gsub("[ \t]{2,}", "", tweets.df)
# # tweets.df = gsub("^\\s+|\\s+$", "", tweets.df) 
#  tweets.df = gsub("coronavirus", "", tweets.df) 
# 
# # corpus will hold a collection of text documents
# tweet_corpus <- Corpus(VectorSource(tweets.df)) 
# tweet_corpus
# inspect(tweet_corpus[1])
# 
# # clean text
# tweet_clean <- tm_map(tweet_corpus, removePunctuation)
# tweet_clean <- tm_map(tweet_clean, removeWords, stopwords("english"))
# tweet_clean <- tm_map(tweet_clean, removeNumbers)
# tweet_clean <- tm_map(tweet_clean, stripWhitespace)
# wordcloud(tweet_clean, random.order=0.5,max.words=100, col=rainbow(50),min.freq = 5,  scale=c(2.0,0.3))


``` 
## test
```{r, eval= TRUE, message=FALSE}
sad=emoList[2]

# load the libraries
library(plyr)
library(stringr)
library(tm)
library(wordcloud)

#library(wordcloud2)

# get the text column
##dataset='mar15.csv'
##Dataset2 <- read.csv(dataset)
##tweets.df<- Dataset2$text
tweets.df<- sad
tweets.df <- sapply(tweets.df,function(row) iconv(row, "latin1", "ASCII", sub=""))

tweets.df = gsub("&amp", "", tweets.df)
tweets.df = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweets.df)
tweets.df = gsub("@\\w+", "", tweets.df)
tweets.df = gsub("[[:punct:]]", "", tweets.df)
tweets.df = gsub("[[:digit:]]", "", tweets.df)
tweets.df = gsub("http\\w+", "", tweets.df)
tweets.df = gsub("[ \t]{2,}", "", tweets.df)
tweets.df = gsub("^\\s+|\\s+$", "", tweets.df) 
tweets.df = gsub("coronavirus", "", tweets.df) 

# corpus will hold a collection of text documents
tweet_corpus <- Corpus(VectorSource(tweets.df)) 
tweet_corpus
inspect(tweet_corpus[1])

# clean text
tweet_clean <- tm_map(tweet_corpus, removePunctuation)
tweet_clean <- tm_map(tweet_clean, removeWords, stopwords("english"))
tweet_clean <- tm_map(tweet_clean, removeNumbers)
tweet_clean <- tm_map(tweet_clean, stripWhitespace)
wordcloud(tweet_clean, random.order=0.5,max.words=100, col=rainbow(50),min.freq = 5,  scale=c(2.0,0.3))

```